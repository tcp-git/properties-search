import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
import argparse
import logging
import math
from typing import Optional, List, Tuple, Dict, Any
from pathlib import Path
import json

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("build_vectorstore")

# Asset type mapping (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embedding text)
ASSET_TYPE_MAPPING = {
    "‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î": "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ä‡∏∏‡∏î",
    "‡∏ö‡πâ‡∏≤‡∏ô": "‡∏ö‡πâ‡∏≤‡∏ô",
    "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°": "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÄ‡∏Æ‡πâ‡∏≤‡∏™‡πå/‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°",
    "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå": "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå"
}


POI_CONFIG = {
    
    # ‡∏£‡∏±‡∏®‡∏°‡∏µ 1 ‡∏Å‡∏°. ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏û‡∏≠‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏±‡πà‡∏á‡∏ß‡∏¥‡∏ô‡∏Ø ‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å ‡∏ñ‡πâ‡∏≤‡πÑ‡∏Å‡∏•‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏∞‡∏£‡πà‡∏ß‡∏á‡∏Æ‡∏ß‡∏ö (Exponential)
    "bts_station": {"radius": 1200, "weight": 1.2, "curve": "exponential"}, 
    "mrt": {"radius": 1200, "weight": 1.2, "curve": "exponential"},
    
    # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏£‡∏ñ‡πÑ‡∏ü/‡∏Ç‡∏ô‡∏™‡πà‡∏á ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•
    "train_station": {"radius": 2000, "weight": 0.5, "curve": "linear"},
    "bus_station": {"radius": 2000, "weight": 0.5, "curve": "linear"},

    
    # ‡∏£‡πâ‡∏≤‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏ã‡∏∑‡πâ‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏ã‡∏∑‡πâ‡∏≠‡∏ô‡πâ‡∏≥‡πÅ‡∏Ç‡πá‡∏á‡πÑ‡∏î‡πâ) 
    "convenience_store": {"radius": 1000, "weight": 0.5, "curve": "exponential"},
    # ‡∏ï‡∏•‡∏≤‡∏î/‡∏ã‡∏∏‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå ‡∏Ç‡∏±‡∏ö‡∏£‡∏ñ‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏®‡∏°‡∏µ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢
    "market": {"radius": 1500, "weight": 0.4, "curve": "linear"},
    "supermarket": {"radius": 2000, "weight": 0.5, "curve": "linear"}, 

    
    # ‡∏´‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏û‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢ (‡∏ï‡∏≤‡∏Å‡πÅ‡∏≠‡∏£‡πå/‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß) ‡πÉ‡∏´‡πâ Weight ‡∏™‡∏π‡∏á
    "shopping_mall": {"radius": 3000, "weight": 0.9, "curve": "linear"},
    # ‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà 2 (Work from Anywhere)
    "cafe": {"radius": 1000, "weight": 0.4, "curve": "linear"},
    "restaurant": {"radius": 1000, "weight": 0.4, "curve": "linear"},
    "community_mall": {"radius": 2000, "weight": 0.6, "curve": "linear"}, 

    
    # ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏á‡∏≠‡∏≤‡∏¢‡∏∏
    "hospital": {"radius": 3000, "weight": 0.7, "curve": "linear"},
    # ‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞/‡∏¢‡∏¥‡∏° ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®
    "park": {"radius": 3000, "weight": 0.6, "curve": "linear"},
    "gym": {"radius": 3500, "weight": 0.5, "curve": "linear"},
    "spa": {"radius":3000, "weight": 0.2, "curve": "linear"},

    
    # ‡∏™‡∏±‡∏ï‡∏ß‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏°‡πà‡πÑ‡∏Å‡∏• ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô
    "veterinary": {"radius": 2000, "weight": 0.5, "curve": "linear"},

    # (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß) 
    "school": {"radius": 3000, "weight": 0.5, "curve": "linear"},
    "university": {"radius": 3000, "weight": 0.3, "curve": "linear"},

    #  NICHE / RELAXATION 
    "river": {"radius": 1500, "weight": 0.4, "curve": "linear"}, 
    "beach": {"radius": 3000, "weight": 0.0, "curve": "linear"},
    "viewpoint": {"radius": 3000, "weight": 0.2, "curve": "linear"},
    "temple": {"radius": 1500, "weight": 0.1, "curve": "linear"}, 
    "museum": {"radius": 5000, "weight": 0.1, "curve": "linear"},
    "tourist_attraction": {"radius": 3000, "weight": 0.2, "curve": "linear"},
    "hotel": {"radius": 2000, "weight": 0.1, "curve": "linear"},
    "golf_course": {"radius": 5000, "weight": 0.2, "curve": "linear"},
}

def fix_asset_type(row):
    """Fix asset type text based on name and description"""
    name = str(row.get('name_th', '')).lower()
    desc = str(row.get('asset_details_description_th', '')).lower()
    eng_name = str(row.get('name_en', '')).lower()
    current_type = str(row.get('fixed_type', '')).strip() # ‡πÉ‡∏ä‡πâ fixed_type ‡∏à‡∏≤‡∏Å CSV ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    
    if current_type and current_type != 'nan':
        return current_type

    if any([
        "condominium" in eng_name or " ‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î" in name or "‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î " in name or
        "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ä‡∏∏‡∏î" in desc or "‡∏´‡πâ‡∏≠‡∏á‡∏ä‡∏∏‡∏î" in desc
    ]):
        return ASSET_TYPE_MAPPING["‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î"]
    
    if any([
        "townhouse" in eng_name or "townhome" in eng_name or
        "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÄ‡∏Æ‡πâ‡∏≤‡∏™‡πå" in name or "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°" in name
    ]):
        return ASSET_TYPE_MAPPING["‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°"]

    if any([
        "commercial" in eng_name or "shophouse" in eng_name or
        "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå" in name or "‡∏ï‡∏∂‡∏Å‡πÅ‡∏ñ‡∏ß" in name
    ]):
        return ASSET_TYPE_MAPPING["‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå"]

    return ASSET_TYPE_MAPPING["‡∏ö‡πâ‡∏≤‡∏ô"]

def compute_poi_percentiles(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    logger.info("Calculating POI percentiles...")
    percentiles_data = {}
    found_cols = 0
    for col_name in POI_CONFIG.keys():
        if col_name in df.columns:
            distances = df[col_name].dropna()
            distances = pd.to_numeric(distances, errors='coerce').dropna()
            
            if not distances.empty:
                percentiles_data[col_name] = {
                    "p10": np.percentile(distances, 10),
                    "p50": np.percentile(distances, 50),
                    "p90": np.percentile(distances, 90),
                }
                found_cols += 1
    
    logger.info(f" Calculated percentiles for {found_cols} POI types.")
    return percentiles_data

def compute_lifestyle_score(row: pd.Series, percentiles: Dict[str, Dict[str, float]]) -> float:
    total_score = 0
    for col_name, config in POI_CONFIG.items():
        if col_name in row and pd.notna(row[col_name]):
            try:
                distance = float(row[col_name])
            except:
                continue 
                
            radius = config["radius"]
            weight = config["weight"]
            
            if distance <= radius:
                score = 0
                if config.get("curve") == "exponential":
                    score = (1 - (distance / radius)) ** 2
                else:
                    score = 1 - (distance / radius)
                total_score += max(0, score * weight)
                
    total_weight = sum(c['weight'] for c in POI_CONFIG.values())
    if total_weight == 0: return 0.0
    
    normalized_score = min(10, (total_score / total_weight) * 10)
    return normalized_score

def extract_features(row: pd.Series) -> Dict[str, Any]:
    features = {}
    features['bedroom'] = row.get('asset_details_number_of_bedrooms', 'N/A')
    features['bathroom'] = row.get('asset_details_number_of_bathrooms', 'N/A')
    
    desc_th = str(row.get('asset_details_description_th', '')).lower()
    desc_en = str(row.get('asset_details_description_en', '')).lower()
    if "‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á" in desc_th or "pet-friendly" in desc_en or "pet friendly" in desc_en:
        features['pet_friendly'] = True
    else:
        features['pet_friendly'] = False
        
    return features

def main(csv_path: str, db_path: str, model_name: str, collection_name: str):
    logger.info(f"üöÄ Starting vector store build from: {csv_path}")
    logger.info(f"üóÇÔ∏è Database path: {db_path}")
    logger.info(f"üì¶ Collection name: {collection_name}")

    # 1. Load Data
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        logger.error(f"‚ùå Error loading CSV: {e}")
        return
    
    logger.info(f"Loaded {len(df)} rows from CSV.")
    
    # 2. Pre-processing
    logger.info("‚öôÔ∏è Processing data and engineering features...")
    
    # Fix asset type (Text)
    df['asset_type_fixed'] = df.apply(fix_asset_type, axis=1)
    
    # Calculate Percentiles & Lifestyle Score
    percentiles = compute_poi_percentiles(df)
    df['lifestyle_score'] = df.apply(lambda row: compute_lifestyle_score(row, percentiles), axis=1)
    
    # Extract other features
    df_features = df.apply(extract_features, axis=1)
    df = pd.concat([df, pd.json_normalize(df_features)], axis=1)
    
    logger.info("‚úÖ Processing complete.")

    # 3. Create Embeddings
    logger.info("Embedding text...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Semantic Search (‡∏£‡∏ß‡∏° ID ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢)
    df['text_for_embedding'] = df['name_th'].fillna('') + " | " + \
                               df['asset_type_fixed'].fillna('') + " | " + \
                               df['asset_details_description_th'].fillna('')
    texts = df['text_for_embedding'].tolist()
    
    logger.info(f"Loading embedding model: {model_name}")
    model = SentenceTransformer(model_name)
    
    logger.info("Generating embeddings... (This may take a while)")
    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)
    logger.info("‚úÖ Embeddings generated.")

    # 4. Prepare Metadata (The Most Critical Part)
    logger.info("Preparing metadata for ChromaDB...")
    
    # ‚úÖ [CRITICAL] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ asset_type_id
    metadata_cols = [
            'id', 'name_th', 'name_en', 'asset_type_fixed', 'province_th', 'district_th',
            'asset_details_selling_price', 'location_latitude', 'location_longitude',
            'asset_details_description_th', 'asset_details_description_en',
            'bedroom', 'bathroom', 'pet_friendly', 
            'lifestyle_score',
            'asset_type_id' # <--- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ 100%
        ]
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° POI columns
    for poi_key in POI_CONFIG.keys():
        metadata_cols.append(poi_key)
        metadata_cols.append(f"{poi_key}_name")

    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô CSV
    final_metadata_cols = [col for col in metadata_cols if col in df.columns]
    
    logger.info(f"Saving {len(final_metadata_cols)} fields to metadata.")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata Dict
    df_metadata = df[final_metadata_cols].copy()
    
    # Handle NaN Values (ChromaDB ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö NaN)
    for col in df_metadata.columns:
        if df_metadata[col].dtype == 'object':
            df_metadata[col] = df_metadata[col].fillna('N/A')
        elif np.issubdtype(df_metadata[col].dtype, np.number):
            if col in POI_CONFIG: # Distance
                df_metadata[col] = df_metadata[col].fillna(99999.0)
            elif col == 'asset_type_id': # ID
                df_metadata[col] = df_metadata[col].fillna(0).astype(int) # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô int
            else:
                df_metadata[col] = df_metadata[col].fillna(0.0)

    metadatas = df_metadata.to_dict(orient="records")
    ids_list = df["id"].astype(str).tolist()

    # 5. Build ChromaDB
    logger.info(f"Setting up ChromaDB client at path: {db_path}")
    client = chromadb.PersistentClient(path=db_path)
    
    # Reset Collection
    try:
        if collection_name in [c.name for c in client.list_collections()]:
            logger.warning(f"Collection '{collection_name}' already exists. Deleting and rebuilding.")
            client.delete_collection(name=collection_name)
    except:
        pass
    
    collection = client.create_collection(name=collection_name)

    # Add to collection
    logger.info(f"Adding {len(df)} documents to collection...")
    
    BATCH_SIZE = 1000
    for i in range(0, len(ids_list), BATCH_SIZE):
        batch_ids = ids_list[i:i+BATCH_SIZE]
        batch_texts = texts[i:i+BATCH_SIZE]
        batch_embeddings_list = embeddings[i:i+BATCH_SIZE].tolist()
        batch_metadatas = metadatas[i:i+BATCH_SIZE]
        
        try:
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings_list,
                documents=batch_texts,
                metadatas=batch_metadatas
            )
            logger.info(f"Added batch {i // BATCH_SIZE + 1} / {math.ceil(len(ids_list) / BATCH_SIZE)}")
        except Exception as e:
            logger.error(f"‚ùå Error adding batch {i // BATCH_SIZE + 1}: {e}")
            logger.error(f"Sample metadata: {json.dumps(batch_metadatas[0], indent=2, ensure_ascii=False)}")

    print("\n" + "="*80)
    print(f"‚úÖ DONE: Vector DB built at {db_path}")
    print(f"   üìä Collection: {collection_name}")
    print(f"   üì¶ Documents: {collection.count()}")
    print("="*80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build Chroma Vector Store")
    parser.add_argument("--csv_path", type=str, required=True)
    parser.add_argument("--db_path", type=str, default="npa_vectorstore")
    parser.add_argument("--model", type=str, default="thenlper/gte-large")
    parser.add_argument("--collection", type=str, default="npa_assets_v2")
    args = parser.parse_args()
    
    main(csv_path=args.csv_path, 
         db_path=args.db_path, 
         model_name=args.model, 
         collection_name=args.collection)