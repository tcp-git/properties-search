import os
from dotenv import load_dotenv 
load_dotenv() 

import json
import logging
import re
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any

import requests
from sentence_transformers import SentenceTransformer
import chromadb

# ============ CONFIGURATION ============
VECTOR_DB_PATH = Path("npa_vectorstore") 
COLLECTION_NAME = "npa_assets_v2" 

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    print("‚ö†Ô∏è WARNING: OPENROUTER_API_KEY is not set.")

EMB_MODEL_NAME = "thenlper/gte-large"
TOP_K_RESULTS = 100 # ‡∏Å‡∏ß‡∏≤‡∏î‡∏°‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏∏‡∏î
FINAL_TOP_N = 5 
LLM_MODEL = "openai/gpt-4o-mini" 

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("search_pipeline")

# ============ PROMPT ENGINEERING ============

ENHANCED_INTENT_DETECTION_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏≠‡∏™‡∏±‡∏á‡∏´‡∏≤‡∏£‡∏¥‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡πÉ‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Query) ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡πâ‡∏≠‡∏ô‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô JSON structure ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

(‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà Query: "{query}" ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)

‡∏à‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Query ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡πâ‡∏≠‡∏ô‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON object ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
{{
  "asset_types": ["‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó1", "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó2", ...],
  "must_have": ["poi1", "poi2", ...],
  "nice_to_have": ["poi1", "poi2", ...],
  "avoid_poi": ["poi1", "poi2", ...],
  "pet_friendly": true/false/null,
  "price_range": {{
    "min": null_or_number,
    "max": null_or_number
  }}
}}

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Field:
1.  "asset_types":
    * ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏≠‡∏™‡∏±‡∏á‡∏´‡∏≤‡∏Ø ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏≠‡∏á‡∏´‡∏≤ (‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
    * ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: ["‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î", "‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß", "‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ù‡∏î", "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°", "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå", "‡∏ó‡∏µ‡πà‡∏î‡∏¥‡∏ô"]
    * ‡∏ñ‡πâ‡∏≤‡∏ö‡∏≠‡∏Å‡∏£‡∏ß‡∏°‡πÜ ‡∏ß‡πà‡∏≤ "‡∏ö‡πâ‡∏≤‡∏ô" ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà: ["‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß", "‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ù‡∏î"]
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô: []
2.  "must_have":
    * POI ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ" (‡πÉ‡∏ä‡πâ POI key ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô: []
3.  "nice_to_have":
    * POI ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ" (‡πÉ‡∏ä‡πâ POI key ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô: []
4.  "pet_friendly":
    * `true` (‡∏ñ‡πâ‡∏≤ "‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå"), `false` (‡∏ñ‡πâ‡∏≤ "‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå"), `null` (‡∏ñ‡πâ‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á")
5.  "price_range":
    * ‡∏ä‡πà‡∏ß‡∏á‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
    * "5 ‡∏•‡πâ‡∏≤‡∏ô" -> 5000000, "10m" -> 10000000, "2.5 ‡∏•." -> 2500000
    * "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏•‡πâ‡∏≤‡∏ô" -> {{ "min": null, "max": 5000000 }}
    * "3-5 ‡∏•‡πâ‡∏≤‡∏ô" -> {{ "min": 3000000, "max": 5000000 }}
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô: {{ "min": null, "max": null }}
6.  "avoid_poi":
    * POI ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£", "‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ", "‡∏´‡∏ô‡∏µ‡∏´‡πà‡∏≤‡∏á" (‡πÉ‡∏ä‡πâ POI key ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
    * ‡πÄ‡∏ä‡πà‡∏ô "‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏´‡∏ô‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏∏‡πà‡∏ô‡∏ß‡∏≤‡∏¢ (market/mall)"
    * ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô: []

[‡∏Å‡∏é POI key ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô]
* "bts", "‡∏£‡∏ñ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡∏ö‡∏µ‡∏ó‡∏µ‡πÄ‡∏≠‡∏™" -> "bts_station"
* "‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô", "7-11", "‡∏£‡πâ‡∏≤‡∏ô‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏ã‡∏∑‡πâ‡∏≠" -> "convenience_store"
* "mrt", "‡πÉ‡∏ï‡πâ‡∏î‡∏¥‡∏ô" -> "mrt"
* "‡∏´‡πâ‡∏≤‡∏á", "‡∏™‡∏£‡∏£‡∏û‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤" -> "shopping_mall"
* "‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏°‡∏´‡∏≤‡∏•‡∏±‡∏¢" -> "school" (‡∏´‡∏£‡∏∑‡∏≠ "university")
* "‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•", "‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å" -> "hospital"
* "‡∏™‡∏ß‡∏ô", "‡∏™‡∏ß‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞" -> "park"
* "‡∏ï‡∏•‡∏≤‡∏î" -> "market"
* "‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£" -> "restaurant"
* "‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà" -> "cafe"

‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
"""

RAG_SYSTEM_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "Mercil" ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏î‡πâ‡∏≤‡∏ô‡∏≠‡∏™‡∏±‡∏á‡∏´‡∏≤‡∏Ø ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏â‡∏•‡∏≤‡∏î
‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" ‡πÅ‡∏•‡∏∞ "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡πâ‡∏≠‡∏ô‡∏°‡∏≤

[‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö]
1.  **‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:** ‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏≠‡∏á‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!
2.  **‡∏û‡∏π‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á:** ‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á" (Penalties) ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏°‡∏±‡∏ô
3.  **‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö:** ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
4.  **‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥:** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô
5.  **‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏µ "‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á" (‡πÄ‡∏ä‡πà‡∏ô 500 ‡∏°.) ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏≠! 

[‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì]
‡∏à‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡πâ‡∏≠‡∏ô‡∏°‡∏≤
"""

def create_rag_user_content(query: str, meta: Dict, reasons: List[str], penalties: List[str]) -> str:
    return f"""
[‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå]
User Query: {query}

Verified Data (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô):
- ‡∏ä‡∏∑‡πà‡∏≠: {meta.get("name_th", "N/A")}
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {meta.get("asset_type_fixed", "N/A")} (ID: {meta.get('asset_type_id', 'N/A')})
- ‡∏£‡∏≤‡∏Ñ‡∏≤: {float(meta.get("asset_details_selling_price", 0)):,.0f} ‡∏ö‡∏≤‡∏ó
- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {str(meta.get("asset_details_description_th", "N/A"))[:150]}

[‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å)]
‚úÖ ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏à (Reasons): {str(reasons) if reasons else "‡πÑ‡∏°‡πà‡∏°‡∏µ"}
‚ö†Ô∏è ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏à (Penalties): {str(penalties) if penalties else "‡πÑ‡∏°‡πà‡∏°‡∏µ"}
"""

# ‚úÖ POI Config (Version 2025 Updated)
POI_CONFIG = {
    # === üöÜ TRANSPORTATION ===
    "bts_station": {"radius": 1200, "weight": 1.2, "curve": "exponential"}, 
    "mrt": {"radius": 1200, "weight": 1.2, "curve": "exponential"},
    "train_station": {"radius": 2000, "weight": 0.5, "curve": "linear"},
    "bus_station": {"radius": 2000, "weight": 0.5, "curve": "linear"},

    # === üè™ CONVENIENCE ===
    "convenience_store": {"radius": 800, "weight": 0.5, "curve": "exponential"},
    "market": {"radius": 1500, "weight": 0.4, "curve": "linear"},
    "supermarket": {"radius": 2000, "weight": 0.5, "curve": "linear"},

    # === üõçÔ∏è LIFESTYLE ===
    "shopping_mall": {"radius": 3000, "weight": 0.9, "curve": "linear"},
    "restaurant": {"radius": 1000, "weight": 0.4, "curve": "linear"},
    "cafe": {"radius": 1000, "weight": 0.4, "curve": "linear"},
    
    # === üè• HEALTH & WELLNESS ===
    "hospital": {"radius": 3000, "weight": 0.7, "curve": "linear"},
    "park": {"radius": 2000, "weight": 0.6, "curve": "linear"},
    "gym": {"radius": 1500, "weight": 0.5, "curve": "linear"},
    "spa": {"radius": 1500, "weight": 0.2, "curve": "linear"},

    # === üê∂ PET FRIENDLY ===
    "veterinary": {"radius": 2000, "weight": 0.5, "curve": "linear"},

    # === üè´ OTHERS ===
    "school": {"radius": 3000, "weight": 0.5, "curve": "linear"},
    "university": {"radius": 3000, "weight": 0.3, "curve": "linear"},
    "river": {"radius": 1500, "weight": 0.4, "curve": "linear"}, 
    "beach": {"radius": 3000, "weight": 0.4, "curve": "linear"},
    "viewpoint": {"radius": 3000, "weight": 0.2, "curve": "linear"},
    "temple": {"radius": 1500, "weight": 0.1, "curve": "linear"},
    "museum": {"radius": 5000, "weight": 0.1, "curve": "linear"},
    "tourist_attraction": {"radius": 3000, "weight": 0.2, "curve": "linear"},
    "hotel": {"radius": 2000, "weight": 0.1, "curve": "linear"},
    "golf_course": {"radius": 5000, "weight": 0.2, "curve": "linear"},
}

# ‚úÖ ASSET ID MAPPING (Corrected)
ASSET_ID_MAPPING = {
    "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏Æ‡∏°": [1],
    "‡∏ó‡∏≤‡∏ß‡∏ô‡πå‡πÄ‡∏Æ‡πâ‡∏≤‡∏™‡πå": [1],
    "‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ù‡∏î": [15], 
    "‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î": [3],
    "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ä‡∏∏‡∏î": [3],
    "‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß": [4],
    "‡∏ö‡πâ‡∏≤‡∏ô": [4, 15], 
    "‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå": [5] 
}

# ============ SERVICE FUNCTIONS ============\

def get_embedding_model(model_name: str) -> SentenceTransformer:
    logger.info(f"Loading embedding model: {model_name}")
    try:
        model = SentenceTransformer(model_name)
        logger.info("‚úÖ Embedding model loaded.")
        return model
    except Exception as e:
        logger.error(f"‚ùå Failed to load embedding model: {e}")
        raise

def get_chroma_collection(db_path: Path, collection_name: str) -> chromadb.Collection:
    if not db_path.exists():
        logger.error(f"‚ùå Vector DB path not found: {db_path}")
        raise FileNotFoundError(f"Vector DB path not found: {db_path}")
    logger.info(f"Connecting to ChromaDB at: {db_path}")
    client = chromadb.PersistentClient(path=str(db_path))
    try:
        collection = client.get_collection(name=collection_name)
        logger.info(f"‚úÖ Connected to collection '{collection_name}' ({collection.count()} documents)")
        return collection
    except Exception as e:
        logger.error(f"‚ùå Failed to connect to collection '{collection_name}'.")
        raise e

def call_openrouter(system_prompt: str, user_content: str, model: str) -> str:
    if not OPENROUTER_API_KEY:
        logger.error("OPENROUTER_API_KEY is not set. Cannot call OpenRouter.")
        return "{}"
    try:
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}]
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}"},
            data=json.dumps({"model": model, "messages": messages})
        )
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content']
        return content
    except Exception as e:
        logger.error(f"Error calling OpenRouter: {e}")
        return "{}"

# ============ SEARCH PIPELINE FUNCTIONS ============\

def enhanced_intent_detection(query: str) -> Dict[str, Any]:
    system_prompt = ENHANCED_INTENT_DETECTION_PROMPT
    user_content = query
    logger.info("Detecting intent...")
    raw_response = call_openrouter(system_prompt, user_content, LLM_MODEL)
    try:
        match = re.search(r'```json\n(.*?)\n```', raw_response, re.DOTALL)
        if match: json_str = match.group(1)
        else:
            json_str = raw_response.strip()
            if not json_str.startswith("{"):
                 start = json_str.find("{")
                 if start != -1: json_str = json_str[start:]
        intent_json = json.loads(json_str)
        validated_intent = {
                "asset_types": intent_json.get("asset_types", []),
                "must_have": intent_json.get("must_have", []),
                "nice_to_have": intent_json.get("nice_to_have", []),
                "avoid_poi": intent_json.get("avoid_poi", []),
                "pet_friendly": intent_json.get("pet_friendly", None),
                "price_range": intent_json.get("price_range", {"min": None, "max": None})
            }
        logger.info(f"Intent detected: {validated_intent}")
        return validated_intent
    except json.JSONDecodeError:
        logger.error(f"Failed to decode JSON from LLM response: {raw_response}")
        return { "asset_types": [], "must_have": [], "nice_to_have": [], "avoid_poi": [], "pet_friendly": None, "price_range": {"min": None, "max": None} }

def chroma_query(collection: chromadb.Collection, embed_model: SentenceTransformer, query: str, k: int, filters: Dict = {}) -> List[Dict[str, Any]]:
    logger.info("Performing semantic search...")
    query_embedding = embed_model.encode([query]).tolist()
    chroma_filter = None 
    if filters:
        filter_list = []
        if "max_price" in filters and filters["max_price"] > 0:
            filter_list.append({"asset_details_selling_price": {"$lte": filters["max_price"]}})
        if "province" in filters and isinstance(filters["province"], str):
            filter_list.append({"province_th": {"$eq": filters["province"]}})
        if filter_list:
            chroma_filter = {"$and": filter_list} if len(filter_list) > 1 else filter_list[0]
    try:
        results = collection.query(query_embeddings=query_embedding, n_results=k, where=chroma_filter, include=["metadatas", "distances"])
        processed_results = []
        if 'ids' not in results or not results['ids']:
            logger.warning("ChromaDB query returned no results.")
            return []
        for i, dist in enumerate(results['distances'][0]):
            meta = results['metadatas'][0][i]
            semantic_score = max(0, 1 - (dist / 2.0))
            processed_results.append({"id": results['ids'][0][i], "semantic_score": semantic_score, "metadata": meta})
        return processed_results
    except Exception as e:
        logger.error(f"‚ùå Error during Chroma query: {e}", exc_info=True)
        return []

def apply_filters(results: List[Dict], filters_cli: Dict, intent: Dict) -> List[Dict]:
    if not filters_cli and not intent.get("price_range"): return results 
    filtered_results = []
    price_range = intent.get("price_range", {})
    final_max_price = filters_cli.get("max_price") if filters_cli.get("max_price") is not None else price_range.get("max")
    final_min_price = price_range.get("min")
    final_province = filters_cli.get("province")
    for r in results:
        meta = r.get("metadata", {})
        keep = True
        price = float(meta.get("asset_details_selling_price", 0))
        if final_max_price is not None and price > final_max_price: keep = False
        if final_min_price is not None and price < final_min_price: keep = False
        if final_province:
            if final_province.replace("‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "").strip() not in meta.get("province_th", "N/A").replace("‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£", "").strip(): keep = False
        if keep: filtered_results.append(r)
    return filtered_results

def compute_intent_match_score(metadata: Dict[str, Any], intent: Dict[str, Any]) -> Tuple[float, List[str], List[str]]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡πÉ‡∏à" (Intent Score) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID ‡πÅ‡∏•‡∏∞ POI Config (Dynamic Radius)
    """
    score = 0.0
    reasons = [] 
    penalties = [] 
    
    # 1. Asset Type Matching (By ID)
    intent_types = intent.get("asset_types", [])
    if intent_types:
        asset_id = metadata.get("asset_type_id", 0)
        asset_type_name = metadata.get("asset_type_fixed", "N/A")
        
        accepted_ids = []
        for t in intent_types:
            accepted_ids.extend(ASSET_ID_MAPPING.get(t, []))
            
        if asset_id in accepted_ids:
            score += 1.0 
            reasons.append(f"‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ({asset_type_name})")
        else:
            score -= 10.0
            penalties.append(f"‡∏ú‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ {', '.join(intent_types)})")

    # 2. Pet-Friendly Matching
    intent_pet = intent.get("pet_friendly") 
    if intent_pet is True: 
        meta_pet_explicit = metadata.get("pet_friendly", False) 
        asset_id = metadata.get("asset_type_id", 0)
        vet_dist = metadata.get("veterinary", 99999) 
        intent_asset_types = intent.get("asset_types", []) 

        if meta_pet_explicit is True:
            score += 1.5; reasons.append("‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÑ‡∏î‡πâ (‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®)")
        else:
            if asset_id == 3: 
                if "‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î" in intent_asset_types:
                    score += 0.0; penalties.append("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• (‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏´‡πâ‡∏≤‡∏°)")
                else:
                    score -= 10.0; penalties.append("‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï)")
            elif asset_id == 4: 
                score += 1.0; reasons.append("‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÑ‡∏î‡πâ (‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß)")
            elif asset_id in [1, 15, 5]: 
                score -= 0.5; penalties.append("‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÑ‡∏î‡πâ (‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÅ‡∏ô‡∏ß‡∏£‡∏≤‡∏ö)")
            else: 
                score -= 10.0; penalties.append("‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå (‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)")

        if vet_dist <= 2000: 
            score += 0.5; reasons.append(f"‡πÉ‡∏Å‡∏•‡πâ ‡∏£‡∏û.‡∏™‡∏±‡∏ï‡∏ß‡πå ({vet_dist:.0f} ‡∏°.)")
    
    elif intent_pet is False: 
         meta_pet_explicit = metadata.get("pet_friendly", False)
         if meta_pet_explicit is True:
            score -= 10.0; penalties.append("‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)")

    # 3. Must-Have POI (Dynamic Radius)
    must_haves = intent.get("must_have", [])
    for poi_key in must_haves: 
        if poi_key in POI_CONFIG:
            distance = metadata.get(poi_key, 99999) 
            poi_name = metadata.get(f"{poi_key}_name", poi_key) 
            limit_radius = POI_CONFIG[poi_key].get("radius", 2000)
            
            if distance <= limit_radius: 
                score += 1.0
                reasons.append(f"‡πÉ‡∏Å‡∏•‡πâ {poi_name} ({distance:.0f} ‡∏°.)")
            else:
                score -= 1.0
                penalties.append(f"‡πÑ‡∏°‡πà‡πÉ‡∏Å‡∏•‡πâ {poi_name} ({distance:.0f} ‡∏°.)")
                
    # 4. Avoid POI (Dynamic Radius)
    avoid_pois = intent.get("avoid_poi", [])
    for poi_key in avoid_pois:
        if poi_key in POI_CONFIG:
            distance = metadata.get(poi_key, 99999)
            poi_name = metadata.get(f"{poi_key}_name", poi_key)
            limit_radius = POI_CONFIG[poi_key].get("radius", 2000)

            if distance <= limit_radius:
                score -= 5.0
                penalties.append(f"‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ {poi_name} ({distance:.0f} ‡∏°.) (‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)")
            else:
                score += 1.0
                reasons.append(f"‡πÑ‡∏Å‡∏•‡∏à‡∏≤‡∏Å {poi_name} ({distance:.0f} ‡∏°.) ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠")
    
    return score, reasons, penalties

def apply_nice_to_have_boost(metadata: Dict[str, Any], intent: Dict[str, Any]) -> Tuple[float, List[str]]:
    nice_boost = 0.0
    nice_reasons = []
    nice_to_haves = intent.get("nice_to_have", [])
    for poi_key in nice_to_haves:
        if poi_key in POI_CONFIG:
            distance = metadata.get(poi_key, 99999)
            poi_name = metadata.get(f"{poi_key}_name", poi_key)
            limit_radius = POI_CONFIG[poi_key].get("radius", 2000)
            
            if distance <= limit_radius: 
                nice_boost += 0.25 
                nice_reasons.append(f"‡∏°‡∏µ {poi_name} ‡πÉ‡∏Å‡∏•‡πâ‡πÜ ({distance:.0f} ‡∏°.)")
    return nice_boost, nice_reasons

def rag_explain_single_item(query: str, intent: Dict, result: Dict, reasons: List[str], penalties: List[str]) -> str:
    meta = result.get("metadata", {})
    system_prompt = RAG_SYSTEM_PROMPT
    user_content = create_rag_user_content(query, meta, reasons, penalties)
    try:
        explanation = call_openrouter(system_prompt, user_content, LLM_MODEL)
        if explanation.strip() == "{}":
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ"
        return explanation.strip().replace('"', '') 
    except Exception as e:
        logger.warning(f"Failed to generate RAG explanation: {e}")
        return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ"

def execute_search(query: str, filters: Dict, embed_model: SentenceTransformer, collection: chromadb.Collection) -> Dict[str, Any]:
    query_intent = enhanced_intent_detection(query)
    results = chroma_query(collection, embed_model, query, TOP_K_RESULTS, filters)
    if not results:
        return { "query": query, "intent_detected": query_intent, "results": [], "message": f"ü§∑ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: \"{query}\"" }
    
    filtered_results = apply_filters(results, filters, query_intent)
    logger.info("Re-ranking results...")
    ranked_results = []
    for r in filtered_results:
        meta = r.get("metadata", {})
        lifestyle_score = float(meta.get("lifestyle_score", 0))
        intent_score, reasons, penalties = compute_intent_match_score(meta, query_intent)
        nice_boost, nice_reasons = apply_nice_to_have_boost(meta, query_intent)
        r["intent_reasons"] = reasons + nice_reasons
        r["intent_penalties"] = penalties
        final_score = ((intent_score * 0.7) + (r["semantic_score"] * 0.2) + (lifestyle_score * 0.05) + (nice_boost * 0.05))
        r["final_score"] = final_score
        r["intent_score"] = intent_score
        r["lifestyle_score"] = lifestyle_score 
        ranked_results.append(r)

    ranked_results.sort(key=lambda x: x["final_score"], reverse=True)
    
    # ‚úÖ [QUALITY GATE] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ! ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏ï‡∏±‡∏î‡∏à‡∏ö‡πÄ‡∏•‡∏¢
    if not ranked_results or ranked_results[0]['final_score'] < 0.35:
        return {
            "query": query,
            "intent_detected": query_intent,
            "results": [],
            "message": "ü§î ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö (Low Matching Score)"
        }
    
    final_results_list = []
    for r in ranked_results[:FINAL_TOP_N]:
        meta = r.get("metadata", {})
        summary_text = rag_explain_single_item(query, query_intent, r, r.get('intent_reasons', []), r.get('intent_penalties', []))
        final_results_list.append({
            "id": r['id'],
            "final_score": round(r['final_score'], 2),
            "intent_score": round(r['intent_score'], 2),
            "summary": summary_text,
            "reasons": r.get('intent_reasons', []),
            "penalties": r.get('intent_penalties', []),
            "asset_details": {
                "name": meta.get('name_th', 'N/A'),
                "price": float(meta.get('asset_details_selling_price', 0)),
                "location": meta.get('province_th', 'N/A'), 
                "bedroom": meta.get('bedroom', 'N/A'),
                "bathroom": meta.get('bathroom', 'N/A'),
                "type_id": meta.get('asset_type_id', 'N/A') 
            }
        })
    
    return { "query": query, "intent_detected": query_intent, "results": final_results_list, "message": "Search completed successfully." }